# 의료인공지능 석사를 마치며

Created: Jan 7, 2021 2:07 AM
Created By: Ryoungwoo Jang
Last Edited By: Ryoungwoo Jang
Last Edited Time: Jan 8, 2021 5:27 AM

2019년에 의대를 졸업하고 서울아산병원, 울산대학교 의과대학에 들어와서 석사를 시작한지 어느덧 2년이 지나 이제 졸업 프로세스를 마치고 졸업을 앞두고 있다. 지난 2년간 정말로 행복했던 시간들이었지만, 그것과 별개로 많은 생각들을 했고 그 생각들을 어느정도 정리해보고 싶어 키보드를 잡았다. 의료인공지능을 시작하려는 사람들에게, 그리고 의료인공지능을 연구하는 사람들에게 조금이나마 도움이 될 글이면 좋겠다. 기술적인 이야기가 주가 될 것이다.

지극히 개인적 관점에서 쓰인 글이니 참고만 하기 바란다.

# 1. 의료인공지능이란?

## 1.1. 의료인공지능의 범위 설정

### 1.1.1. 의료인공지능의 범위 설정

의료인공지능의 가능성을 논하기 이전에, 의료인공지능이 무엇인지부터 정의를 하고 가야 할 것이다. 의료라는 단어의 범위는 생각보다 명료하지만, 인공지능이라는 단어의 범위는 사람에 따라 매우 폭넓게 갈린다. 하지만 이 글에서는 **인공지능**이라는 기술은 **딥러닝**이라는 기술을 사용하고 응용하는 것에 한정짓는다. 딥러닝이 무엇인지는 다들 알거라 간주하고 패스.

### 1.1.2 의료인공지능의 예시

딥러닝 모델은 다양한 기준과 다양한 관점에서 분류를 할 수 있지만, 그런 분류법은 여기서는 논외로 하고 내가 말하고자 하는 관점에 따라 간단하게나마 분류를 기술해보면 다음과 같이 분류할 수 있을 것이다.

1. 지도학습
    1. 분류 모델
    2. 분할(segmentation) 모델
    3. 검출(detection) 모델
2. 비지도학습
    1. 적대적 생성 신경망(GAN)
    2. 오토인코더(Autoencoder)

지도학습과 비지도학습에 대해서 나는 학위 과정을 하며 어떻게 느꼈는지 하나하나 풀어보면 아래와 같다.

# 2. (지극히 개인적인) 전략들에 대한 입장

## 2.1. 지도 학습

가장 기초이자 가장 쉬운 동시에 가장 어렵고 가장 많이들 시도하는 전략이다. 쉽게 말해 질환이 있으면 그 질환에 대한 정답지를 주고 컴퓨터에게 질환과 정상을 번갈아가며 학습시키면서 차후에 새로운 사진이 들어왔을 때 질환인지 아닌지 잘 분류하고, 잘 분할하고, 잘 검출하도록 하는 방법론이다. 의료인공지능에서 지도학습 모델 중 가장 임팩트를 준 시초 논문은 아마 [구글의 diabetic retinopathy (당뇨병성 망막병증) JAMA 논문](https://jamanetwork.com/journals/jama/fullarticle/2588763)과 그 후속 연구가 아닐까 한다. 분할이랑 검출은 내가 잘 몰라서 그런지 모르겠는데 실제로 임상적으로 큰 화두가 되었던 연구는 아직 못찾았기 때문에 패스.

시작하기 전에 diabetic retinopathy (당뇨병성 망막병증)은 당뇨 환자에게서 눈에 오는 합병증이다. 당뇨는 혈당이 올라가는 질환이고, 고로 전신의 모든 혈관을 다 침범하는 질환인데 이 때 침범 속도는 혈관마다 달라서 모세혈관부터 망가지기 시작한다. 바로 눈이 그 모세혈관 중에 우리가 직접 볼 수 있는 유일한 곳이라서 의사들이 당뇨병성 망막병증을 주의깊게 살펴보는 것이고, 실명에까지 이를 수 있기에 더더욱 주의깊게 살펴볼만한 가치가 있게 된다.

논문을 읽어보면 알겠지만 이 논문이 좀 유감스러운 이유는 구글만이 할 수 있는 연구였기 때문이다. 논문에 나온 바에 따르면 2015년 5월부터 2015년 10월까지 multi-center (다기관)에서 수집한 12만 8175장의 이미지들을 사용했다. 여기까지는 좀 large scale연구를 할 수 있는 기관이라면 해봄직한 연구니 OK. 진정으로 구글만이 할 수 있었던 이유는 54명의 안과의사와 안과 4년치 레지던트가 이 사진들을 모조리 점수메겼기 때문이다. 그 비싼 미국 의사 인건비를 54명이나 감당하며 연구할 의지가 있는 기업은 구글밖에 없을 것이다.

뭐 어쨌든 이렇게 모은 귀한 데이터셋으로 학습시킨 딥러닝 모델을 검증까지 꼼꼼하게 마치고, 구글이 했던 것은 real world, 그 중에서도 제3세계에서 이 알고리즘을 검증하는 것이었다. [Nature NPJ Digital Medicine의 구글 논문](https://www.nature.com/articles/s41746-019-0099-8)이 바로 그 논문인데, 여기에는 사회, 경제, 문화적 요소들이 숨어있다.

태국은 약 7000만 근방의 인구의 나라인데, 7000만명의 전국 인구 중에 겨우 1500명이 안과 의사이고, 200명이 망막 전문의(retinal specialists)라고 한다. 태국의 당뇨 환자 수는 약 450만명 정도로 추정되는데, 문제는 안과 의사의 절반과 망막 전문의의 절반 가량이 수도인 방콕에서 의술을 펼친다는 것이다. 고로, 애초에 안과 의사와 망막 전문의가 극히 드물 뿐더러 수도와 시골의 의료격차가 심각하기 때문에 자동화된 망막 사진 판독 알고리즘의 needs가 있기 때문에 구글은 태국을 선택한 것이다.

뭐 당연히 결과야 좋았으니 논문 썼겠다고 생각하고 결과의 디테일은 넘어가자. 여기서 우리는 무엇을 생각해볼수 있을까? 내가 생각하기에 이 두 연이은 연구가 시사하는 것은 몇 가지가 있다.

1. 구글만이 할 수 있는 large scale의 비싼 연구가 시사하는 바는? 왜 구글은 이렇게 비싼 연구를 했을까?
2. 왜 하필 당뇨병성 망막병증을 골랐을까? 지도학습에 있어서 목표하는 질환을 고르는 기준은?
3. 제3세계를 고른 이유는?

내가 생각하는 지극히 주관적인 답들은 다음과 같다.

1. 구글만이 할 수 있는 large scale의 비싼 연구가 시사하는 바는? 왜 구글은 이렇게 비싼 연구를 했을까?
    - 지구 최고의 인공지능 연구 집단인 구글 브레인을 갖고 있는 구글도 고민을 엄청 많이 했을거라는 가정 하에 푸는 논리(라고 쓰는 뇌피셜)는 다음과 같다. 아마 구글도 소량의 데이터를 가지고 어떻게 처음에는 연구를 해보려고 했을 것이다. 하지만 눈 사진을 찍는 기계의 다양성, 환자들의 다양한 질환의 정도, 해부학적 특성, 기타 수많은 요인들을 모른체 하고 넘어갈 수도 없었을 것이다. 그렇기 때문에 결국 12만장이 넘는 안저 사진을 모았을 것이다.
    - 사실 내가 생각했을 때 의료 영상을 지도학습으로 가르키는 요즘 분위기에는 원죄처럼 작용하는 논문이 하나 있다. 바로 [Deep Learning is Robust to Massive Label Noise](https://arxiv.org/abs/1705.10694)라는 논문인데, 이 논문의 골자는 정답이 좀(사실은 많이) 틀려도 딥러닝은 알아서 잘 배우고 정확도가 얼마 떨어지지 않는다는 논문이다. 이 논문을 근거로 의료인공지능 연구자들은 판독문 기반 정답 부여를 하고는 하는데, 이건 무슨 말이냐면 의사가 다시 재판독하기 귀찮고 다시 판독하면 시간도 또 엄청 걸리니까 예전에 판독문을 적어 놓은 것을 좀 잘 활용해서 어떻게 인공지능 모델을 만들어 보자는 일종의 학계의 우회로같은 컨센서스가 형성되어 있는 것이 현실이다. 이 컨센서를 이끌고, 활용한 연구들이 National Institute of Health (NIH)의 [Chest X-Ray 논문](https://arxiv.org/pdf/1705.02315.pdf)이고, 이와 아주 유사한 논문이 Stanford의 [CheXpert dataset 논문](https://arxiv.org/abs/1901.07031)이다. 심지어는 위의 [Deep Learning is Robust to Massive Label Noise](https://arxiv.org/abs/1705.10694) 논문을 인용하면서 "딥러닝은 정답이 좀 틀려도 잘하니까 우리는 판독문 기반으로 대충 정리해서 large scale의 연구를 했어~"라는 [연구](https://www.sciencedirect.com/science/article/abs/pii/S1361841518304997?via%3Dihub)도 있다. [내 논문](https://medinform.jmir.org/2020/8/e18089?utm_source=TrendMD&utm_medium=cpc&utm_campaign=JMIR_TrendMD_0)이 저 주장을 정면으로 반박하는 논문이고(많이 citation 해주세요 ㅎㅎ). 어찌됐든, 구글은 저런 연구들을 안믿는것 같다. 만약 믿었으면 그냥 안저 사진 판독문 기반으로 분류해서 비용을 엄청나게 절감했겠지. 54명에게 새로 레이블을 달라고 시키고, 재판독하는건 정말로 깨끗한, 정확한 정답을 만들겠다는 의지로밖에 보이지 않는다.
    - 결국, 위 두 장황한 문단들을 요약하자면 (1) 이미지들의 특성이 다른 것을 극복하고자 함. (2) 정확한 정답을 갖고자 함. 이 될 것이다. 이것이 시사하는 바는 다음처럼 정리할 수 있다.
    - 이미지들의 픽셀 단위의 특성은 기기마다, 조건마다 다르기 때문에 지도학습을 하려면 이것이 극복되어야 한다. A라는 기기에서 찍은 영상으로 학습시켜서 B기기로 찍은 영상에 테스트하면 정말 이상한 결과가 나오는 경우가 부지기수이기 때문이다. 이를 인공지능 분야에서는 Domain adaptation 문제라고 부른다. Domain adaptation 문제를 해결하기 위해서 구글은 저렇게 많은 데이터셋을 구성한 것이다.
    - 지도 학습에서는 정확한 정답이 중요하다고 생각할 수밖에 없다. 아무리 연구 결과들이 "정답지가 좀 틀려도 괜찮아"라고 말한다 하더라도 불안해서 어떻게 그걸 믿나. Garbage in, garbage out이 아닐까 한다.
2. 왜 하필 당뇨병성 망막병증을 골랐을까? 지도학습에 있어서 목표하는 질환을 고르는 기준은?
    - 이건 사람마다 정말 생각이 다를 수 있는데 크게는 두 가지 입장으로 나눌 수 있다. (1) 실제 의료 현장에서 필요로 하는 인공지능 알고리즘을 만들자. (2) 우리가 어렴풋하게나마 그렇지 않을까? 하고 추측만 하고 있었던 상관관계를 학습시켜 보자.
    - (1)번은 필드에서 뛰는 임상 의사들의 큰 관심사이고 대부분의 연구들이 (1)번 기준으로 이뤄진다. (1)은 반박의 여지 없이 중요한 주제이다. 그렇다고 (2)가 의미가 없는 것은 전혀 아니다. 비싼 검사 대신 싼 검사를 진단 스크리닝 목적으로 사용한다던지, 정확도가 좀 떨어지더라도 시간이 오래 걸리는 검사의 시간을 줄여준다던지 하는 다양한 장점들이 있을 수 있기 때문이다.
    - 결국 목표하는 것이 어떤 것인지에 따라 다를 수 있을것 같다. 진짜 의료 현장에 도움을 주는 practical한 무언가를 하느냐, 아니면 우리의 지식의 외연을 넓히느냐.
3. 제3세계를 고른 이유는?
    - 항상 인공지능은 학습이 잘 되느냐는 기본이고, 실제 세계에 얼마나 잘 맞아떨어지느냐에 대한 검증을 필요로 한다. 따라서 실제 세계가 학습 과정의 데이터와 다른데도 적용이 잘 된다면 그건 정말 의미있는 인공지능 모델이라고 할 수 있다. 이러한 맥락에서 구글은 백인종에 대해 학습시키고 황인종에 대해서 검증을 한 것이라고 생각할 수 있다.

지도학습은 장단점이 명확한 기술이다. 정말 특정한 task를 하기 위해서는 당연히 지도학습을 사용해야 하지만, 위에서 내 의견을 피력했듯 지도학습은 정확한 정답을 필요로 하고, 시킨 것밖에 하지 못하며, 새로운 일을 하기 위해서는 처음부터 모델을 다시 학습해야 한다는 한계점이 있다.

## 2.2. 비지도학습

비지도학습은 정답지가 없는 인공지능 학습이다. 가장 대표적인 비지도학습의 몇 가지 예시는 k-최근접 이웃(kNN) 학습법, 적대적 생성 신경망(generative adversarial network; GAN), 자기지도학습(self-supervised learning) 정도가 아닐까 싶다.

1. GAN은 2014년에 [Ian Goodfellow의 논문](https://arxiv.org/abs/1406.2661)이 나온 이래로 활발하게 연구되는 알고리즘인 것은 다들 알고 있을 것이다. 소위 말해 가짜 영상을 컴퓨터가 생성하는 GAN이 나는 내 생각보다 저평가받고 있다는 생각을 많이 한다. 진짜처럼 보이는, 영상의학과 의사도 구분하지 못하는 가짜 Chest X-Ray (CXR) 영상을 생성해서 어디다가 쓸건가? 임상적 응용이 어디에 있을 것인가? 라는 질문을 하는 것이 당연하지만, 내 주관적인 생각으로는 사람들의 상상력이 부족하지 않나 싶다.

    GAN은 최초에는 28$\times$28 영상 정도밖에 생성하지 못하다가 [PGGAN](https://arxiv.org/abs/1710.10196), [StyleGAN](https://arxiv.org/abs/1812.04948), [StyleGAN2](https://arxiv.org/abs/1912.04958)와 같은 네트워크들이 개발되며 1024$\times$1024정도의 realistic한 영상을 생성할 수 있게 되었다. ([컴퓨터가 생성한 가짜 영상 demo 비디오](https://www.youtube.com/watch?v=9QuDh3W3lOY))

    가짜 CXR 영상을 생성만 하는 것에는 아무런 의미가 없다. 그것을 임상에 응용하는 것은 다른 상상력을 필요로 한다. 여기서 기가 막힌 논문이 하나 나온다. [AnoGAN](https://arxiv.org/abs/1703.05921)이라고 불리는 이 시초 논문의 아이디어는 다음과 같다. 먼저 (1) 정상 사진을 엄청난 양으로 GAN에게 학습시켜서 GAN이 정상 이미지만 생성할 수 있게 한다. (2) 그런 다음 real-world setting에서 비정상 사진이 들어오면 그 비정상 사진과 제일 비슷한 정상 버전의 가짜 이미지를 만든다. (3) 그리고 마지막으로 비정상 사진에서 정상 버전의 가짜 이미지를 빼서 질환만 남기는 방식으로 질환을 검출해준다. 이 방식은 당연히 정답이 필요없다. 나는 개인적으로 이 알고리즘 scheme을 처음 접하고 머리를 한 대 맞은 느낌이 들 정도로 창의적인 아이디어라는 생각이 들었다.

    하지만 실제로 이 알고리즘을 적용하는 것은 당연히 쉽지 않다. 실제로 정상 버전의 가짜 이미지라는 것은 매우 만들기가 힘들다. Brain tumor (뇌종양)을 생각해보자. 뇌종양이라는 비정상 이미지에 대해서 그것의 정상 버전은 무엇일까? 당연히 종양이 없는 것이라고 생각할 수 있지만 그게 그렇게 간단치가 않다. 뇌종양이 점점 커짐에 따라서 뇌는 반대쪽으로 밀리게 된다. 이러한 현상을 mass effect라고 한다 ([그림 참조](https://www.pinterest.co.kr/pin/155303887191202556/)). 문제는 이렇게 한쪽으로 뇌가 밀려버리면 정상 버전의 영상을 만들기가 매우 까다로워진다는 것이다. Mass effect가 일어난 뇌의 정상 버전은 무엇일까? [한쪽으로 밀리고 종양이 없는 뇌]는 아니기 때문에 [밀리지 않은, 대칭인 정상 뇌 영상]을 생성해야 하는데, 이미 비정상 사진은 뇌가 한쪽으로 밀려 버렸기 때문에 [뇌가 한쪽으로 밀린 종양이 있는 사진] - [밀리지 않은 정상 뇌]를 해버리면 종양 뿐만 아니라 다른 구조물들도 비정상으로 마킹이 되어 버린다. 이것을 AnoGAN에서 나타나는 false positive 문제라고 하고 이 false positive를 줄이는 것이 매우 중요해진다. 이러한 난관들은 생각보다 해결하기가 매우 까다롭고 AnoGAN은 그 이상적 가정에 비해 현실적 문제들에 부딪혀 매우 어려운 문제로 돌변해버린다.

    이러한 AnoGAN적 접근 이외에도 GAN을 사용할 수 있는 대표적 분야는 style transfer이다. Style transfer이라는 것은 사진의 특정 스타일을 다른 스타일로 바꿔주는 것을 의미하는데, 예를 들어 말을 얼룩말로 무늬만 바꿔주는 등의 기술을 뜻한다. 이 중 가장 대표적인 알고리즘이 CycleGAN이 되고([CycleGAN 공식 페이지](https://junyanz.github.io/CycleGAN/). Demo 영상들을 여기서 확인할 수 있다) 이를 변형한 다양한 알고리즘들이 제안되었고 제안되고 있다. 간단하게만 말하면 $X$라는 도메인에서 (말) $Y$라는 도메인으로 (얼룩말) 변환을 하고 싶다고 할 때 CycleGAN의 저자들이 쓴 전략은 $F:X\to Y$로 스타일을 바꿔주는 네트워크가 있다고 하고 $G:Y\to X$로 바꿔주는 네트워크가 있다고 할 떄 이미지 $x\in X$에 대해서 $G(F(x))\approx x$가 되도록, 그리고 마찬가지로 $y\in Y$에 대해서 $F(G(y))\approx y$가 되도록 학습을 시키자는 전략을 취해주는 것이다. 즉, 스타일을 갔다 오는 것이 자기 자신과 유사해지도록 만들어보자는 것이 된다. 그렇다면 이것을 어디에 쓸 수 있을까? 가장 대표적인 예시가 CT (computed tomography) style transfer이다.

    CT영상은 대충 보기에는 동일해 보이지만 실제로는 매우매우 다르다 (heterogeneous하다). CT physics를 조금 얘기해보면 CT는 일반 PNG, JPEG영상과는 다르게 DICOM이라는 확장자를 갖고 있다. DICOM은 국제 표준이 있긴 하지만([링크](https://www.dicomstandard.org/)), 그게 또 말처럼 잘 통일되어 있지가 않다. DICOM은 PNG, JPEG의 8-bit 이미지가 아닐 수도 있다. 여기서 8-bit 이미지라는 것은 이미지의 픽셀 값이 0부터 255($=2^8-1$) 사이의 범위를 갖는 이미지를 말한다. DICOM은 12비트 이상의 이미지를 사용하고, 특히나 CT의 경우 많은 이미지들이 12-bit 이미지로 픽셀값은 -1024부터 3071까지의 값을 갖는다. CT 국제 표준은 Hounsfield unit (HU)을 사용하는 것이 standard인데, 이 HU는 공기의 픽셀값을 -1024로 두고, pure water의 픽셀값을 0으로 둬서 이 사이를 linear하게 등간격으로 나눠서 픽셀 값을 할당하는 방식을 채택한다. ([HU wikipedia 설명](https://en.wikipedia.org/wiki/Hounsfield_scale)) 문제는 이게 이론적으로는 그렇다고 쳐도 실제 물리적 구현이 회사마다 달라서 조금씩 픽셀 값들에 차이가 나게 된다. 뿐만 아니라, CT 기계가 찍은 원본 영상인 [sinogram](https://en.wikipedia.org/wiki/Radon_transform#/media/File:Sinogram_-_Two_Square_Indicator_Phantom.svg)(sine파처럼 생겨서 sinogram이라고 부른다.)을 우리가 보는 이미지인 CT로 바꿔주는 과정에서 들어가는 수많은 전처리에 따라서 noise level이 달라지고, 이미지의 스타일이 많이 바뀌게 된다. 뿐만 아니라 조영제 주입량이나 방사선 조사량이 이미지마다 또 다를 수 있기 때문에 역시나 이미지의 스타일이 바뀐다.

    이러한 이미지의 스타일을 통일해주는데 앞서 언급한 CycleGAN의 개량 모델들이 많이 개발되었다. 특히나 KAIST의 예종철 교수님께서 이러한 연구를 매우 활발히 하시는데, low-dose CT와 standard-dose CT의 스타일을 바꿔주는 (노이즈를 줄여주는=denoising) [연구](https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.13284)도 있고 (당연히 low-dose에서 standard-dose로 가는 방향이 중요하다.), CycleGAN의 SOTA (State-of-the-Art, 최고 성능을 보이는 모델)라고 개인적으로 생각하는 [OT-CycleGAN](https://arxiv.org/abs/1909.12116) (Optimal Transport-driven CycleGAN)과 같은 모델들이 CT style transfer를 가능케 해준다. 

    이러한 style transfer는 특정 기기 회사에서만 학습된 모델을 다른 회사의 영상이 들어와도 정확한 결과를 낼 수 있도록 해준다. 예를 들어 아산병원에서는 Siemens사의 영상이 대부분을 차지하는데, 내 모교인 영남대는 Phillips사의 영상을 쓴다. 그렇다면 영남대의 영상을 아산병원에서 학습한 모델로 테스트하고 싶다고 할 때 이러한 style transfer이 중요해지게 되는 것이다.

2. 요즘 매우 핫한 자기지도학습(self-supervised learning; SSL)은 여러가지 세부 알고리즘으로 나뉜다. 먼저 [SimCLR](https://arxiv.org/abs/2002.05709), [MOCO](https://arxiv.org/abs/1911.05722), [MOCO v2](https://arxiv.org/abs/2003.04297), [RELIC](https://deepmind.com/research/publications/Representation-Learning-via-Invariant-Causal-Mechanisms),  [Propagate yourself](https://arxiv.org/abs/2011.10043)같이 한 번에 모든 것을 다 하려고 하는 모델이 아닌 upstream task를 잘 하려는 contrastive learning을 담당하는 한 축이 있는가 하면, [Variational AutoEncoder](https://arxiv.org/abs/1312.6114)를 기반으로 하는 latent vector를 잘 encoding하려는 다른 축이 있다. (대표적인 두 축만 나열한 것이고 SSL의 철학을 담은 알고리즘들은 매우 많다.) 중요한 점은 이 두 축 모두 이미지의 특성을 잘 배우는 네트워크를 학습하겠다는 목적을 가진다는 것이다. 철학이 미묘하게 다르긴 한데 말로 표현하기는 좀 어려워서 여기까지만 기술적인 설명을 하는걸로 하고, 임상 응용 가능성은 두 축 다 이미지를 잘 압축해서 이미지에서 중요한 정보는 살리고 중요하지 않은 정보는 대충 날려버리겠다는 철학을 가지고 있으니, 1024$\times$1024$\approx$100,0000 차원의 이미지를 더 낮은차원으로 끌어내려 다양한 downstream task를 풀겠다는 이야기로 환원될 수 있다. 자세한 얘기는 embargo.

위에서 "지도학습은 장단점이 명확한 기술이다"라고 했는데 비지도학습 또한 마찬가지이다. 정답이 필요없다는 것은 노동력을 혁신적으로 줄여주는 장점이 되지만, 비지도 학습의 가장 큰 단점은 lung nodule (폐결절)과 tuberculosis (결핵) 처럼 비슷한 특징을 구분해주지 못한다는 것이다. 비지도학습은 "이 부분은 이상합니다"라고만 해주지 "이 부분은 폐결절이지 결핵이 아닙니다"라는 말을 해주지 못한다. 이는 결국 의사가 영상을 보고 판독을 어찌됐든 하기는 해야 한다는 결론에 도달하고, 특수한 경우가 아니면 의사의 업무 효율을 놀랍도록 단축해주지 못할 것이라는 추측을 할 수 있다(이 부분은 내 개인적 생각이다. 아닌 경우도 분명히 존재하기에 "특수한 경우가 아니면"이라는 단어를 사용했고 기술의 발전이나 임상적 통찰력을 통해 이 문장이 틀린 문장이 될 수도 있다.).

# 3. 총평, 의료인공지능이 나아가야 할 길

위에서 지도학습과 비지도학습에 대해서 내가 생각하기에 중요한 포인트들과 연구가 핵심적으로 이루어져야 하는 방향을 장황하게 적었는데, 전적으로 나의 개인적 생각이고 내가 놓쳤거나 생각지도 못한 부분이 많을 것이다.

의료인공지능은 나는 이제 시작인 새로운 학문이라고 생각한다. 딥러닝이 뜨기 전의 CAD (Computer-aided Diagnoisis)와는 차원이 다른 새로운 길을 걷고 있지만 아직도 할 일이 너무나도 많다고 생각한다. 특히나 내가 생각하기에(라기보다는 영상의학과 교수님들의 insight를 빌려 말해보자면) 의료인공지능에서 중요한 문제들은 다음 리스트를 포함하고 있지 않을까 싶다.

1. 영상 촬영 장비의 차이를 극복할 수 있는 robust learning algorithm 개발
2. 대량의 데이터가 아닌 소수의 데이터로도 충분한 이미지의 representation을 학습(representation learning with few-shot algorithm)
3. 영상의학과, 임상 의사들의 workflow를 개선해 줄 수 있는 임상 의사와 인공지능 연구자의 원활한 협업
4. General하게 성능이 좋은 알고리즘이 아닌, 문제-specific한 algorithm 개발, 개량.
5. (조금 상상력을 보태 보자면) Heterogeneous treatment effect (환자마다 치료 효과가 달라지는 현상)을 다룰 수 있는 통계적 알고리즘에 기반한 인공지능 알고리즘. ([예시](https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839?journalCode=uasa20)) Causal learning이 부분적인 해답을 줄지도.
6. Multi-Omics. 영상정보와 임상 정보, 유전체 정보를 통합하여 판단하는 알고리즘의 개발.

이렇게 거대한 의료인공지능이라는 분야에서 내가 벽돌 하나를 쌓을 수 있기를 바랄 뿐이다.