---
title: "Radiologist vs AI?"
tags:
  - deep learning
  - radiology
  - artificial intelligence
  - neural network
use_math: true
--- 

유방촬영술(mammography)에서 인공지능 알고리즘이 의사보다 유방암을 찾는 성능이 더 좋다는 네이처에 출간된 구글 딥마인드의 논문이 화두이다.

기자들이 오버하는 거겠지 싶어서 별 관심이 없었는데, 하도 말이 많아서 직접 논문을 읽어보았다.

[International evaluation of an AI system for breast cancer screening](https://www.nature.com/articles/s41586-019-1799-6)

남들이 느낄 수 있는 문제야 그렇다 치고, 내가 의료인공지능을 하는 입장에서 느낀 문제는 뭐냐면, 딥러닝을 사용해서 의사보다 성능이 좋다는 인공지능 연구는 많이 있어 왔다는 것이다.

내가 본 것만 해도

1. Andrew ng - 폐렴(pneumonia)
트위터에 "Should radiologists be worried about their jobs? Breaking news: We can now diagnose pneumonia from chest X-rays better than radiologists." 라고 올리며([트위터 링크](https://twitter.com/andrewyng/status/930938692310482944)) 퍼블리시한 논문이 CheXNet이라는 네트워크(DenseNet의 변형)이고, ([논문 링크](https://arxiv.org/abs/1711.05225)) 특이한 점은 열몇개의 질환군 중에서 폐렴만 사람보다 잘한다고 홍보를 한다.

2. 박창민 교수님 - 폐결절(lung nodule)
훌륭한 연구를 많이 해오신 서울대병원 영상의학과 박창민 교수님의 연구에서도 이러한 맥락을 읽을 수가 있는데 ([논문 링크](https://doi.org/10.1148/radiol.2018180237)) 이 논문에서는 재미있게도 흉부 판독에 대한 전문성이 떨어질수록 인공지능 알고리즘의 동움을 많이 받는다는 것을 실험적으로 보였다. 즉, 영상의학과 의사가 아닌 사람이 제일 도움을 많이 받고, 흉부 전공이 아닌 영상의학과 의사는 도움을 적당히 받고, 흉부 전공 영상의학과 의사가 도움을 가장 적게 받는다고 한다. 어찌 보면 당연한게, 정답 레이블이 흉부 전공 영상의학과 의사가 한거니 그럴 수밖에 없어 보인다.

3. Majkowska et al
얼마 전에 출간된 Radiology 저널의 논문도 비슷한 주장을 한다. ([논문 링크](https://doi.org/10.1148/radiol.2019191293)) 여기에서는 딥러닝 알고리즘이 일반영상의학과 전문의 정도의 수준까지 훈련될 수 있다고 하는데 특이한 점은 흔하게 발견되는 네 개의 중요한 질환(기흉, 폐결절, 폐음영(opacity), 갈비뼈 골절)에 대해서 모두 영상의학과 의사만큼 성능을 끌어올렸다는 것이다.

그리고 기타등등이 있었다.(더 나열하면 안읽을거같아서 3개만 예시를 들었음.)

진짜 딥러닝 알고리즘은 내공 20년의 의사만큼 판독을 정확하게 할 수 있는 것일까?

나는 감히 아니라고 주장하고 싶다.

그 이유는 다음과 같다.

ㄱ) 영상에서 얻을 수 있는 정보의 한계점

영상의학이라는 분야가 생긴 이래로 수많은 영상의학 소견들이 발견되고 발전되어 왔음에도, 영상만으로 모든 것을 판단하지는 않는다. 물론 영상만으로 진단되는 질환(ex 기흉)이 있는건 의심할 수 없는 사실이지만 모든 질환을 영상 하나만 갖고 판단할 수 있는건 아니다. 특히나 암의 경우에는 병리학적인 진단이 필수인데 이러한 부분은 여러 과의 협업이 필요하기도 하고, 의사 국가고시를 공부하면 중요하게 배우는(하지만 문제는 쉽게 나오는) CT상에서 우연히 발견된 SPN(Solitary Pulmonary Nodule, 단일폐결절)의 경우에도 [Fleishner society 2017 guideline](https://doi.org/10.1148/radiol.2017161659)에 따르면 가장 기본적인 플랜은 몇 달 기다렸다 또 찍어서 커지는지 유지되는지를 살펴보자는 것이다. 즉, 영상을 한 번 찍어서 바로 모든 정보를 다 알수는 없다는 것을 의미한다.

ㄴ) 딥러닝 알고리즘의 본질적 문제점

이러한 사실로부터 딥러닝 알고리즘이 영상의학과 의사보다 잘 한다는 것은 무엇을 의미하는지를 생각해 봐야 한다. 진정으로 인류가 찾아내지 못한 새로운 영상의 특성을 관찰하고 있는 것일수도 있지만, 지난 100년간 어지간한 영상 소견들은 영상의학과 선생님들께서 다 찾아내었기 때문에 나는 그럴 가능성이 높지 않다고 생각한다. 오히려 우리가 딥러닝에게 이상한 정보만 주고 있을지도 모른다는 것이다.

예를 들어, 잘 알려진 문제지만 정상 999장, 비정상 1장을 딥러닝 모델에게 보여주면 딥러닝 모델은 무조건 정상으로만 판독해도 정확도가 99.9%이기 때문에 아예 학습을 하지 않고 편하게 무조건 정상으로만 판독하려고 한다.

다른 문제로, 이건 의료인공지능을 해본 사람만이 알 수 있을 문제일것 같은데 흉부 x선 사진같은 기기에서는 우측에 심장이 있는 환자와 같은 경우(Dextrocardia)에 사진의 좌우가 뒤바뀌는 것을 방지하기 위해서 L이나 R이라는 인공 구조물을 사진에 끼워넣는다.([사진](https://en.wikipedia.org/wiki/Chest_radiograph#/media/File:Implantable_cardioverter_defibrillator_chest_X-ray.jpg) : 여기서 보이는 뒤집어진 L자와 같은 것이 그러한 것.) 문제는 병동(환자의 비율이 높음)의 기기에는 R자가 들어간 기계를 쓰고 건강검진센터(정상 소견의 비율이 높음)의 기기에서는 L자가 들어간 기계를 써버리면 딥러닝 알고리즘은 야비하게도 L이 들어가면 정상, R이 들어가면 비정상으로 분류하는 꼼수를 배워 정확도를 높인다(Grad-CAM이라는 알고리즘으로 분류기가 무엇을 보고 있는지 직접 확인이 가능하다.). 

이러한 문제들뿐 아니라 실제로 딥러닝이 꼼수를 써가며 배우는 것을 막기위해 수많은 혼란을 알고리즘에게 주어야 한다.

ㄷ) '잘 한다'는 것은 무엇일까?

다시 윗윗윗문단으로 돌아가서 "딥러닝 알고리즘이 영상의학과 의사보다 잘 한다는 것은 무엇을 의미하는지"를 생각해 보면, 딥러닝은 의사의 조심스러움을 과감하게 버린다는 것을 의미한다는 해석을 할 수 있다. 최소한 위에서 언급한 3개의 연구에서 '이거 이상한데 아직은 확신을 못하겠으니까 3, 혹은 6달 뒤에 다시 찍어서 변하는지 한번 볼까?' 같은 고민은 들어가지 않았다.(연구 디자인이 잘못되었다는 것이 아니라, 이러한 조심성을 컴퓨터에게 가르쳐준다는 것은 너무나도 어려운 일임을 의미하는 것이니 오해 마시길.) 딥러닝 알고리즘은 그저 사진을 주면 일련의 계산을 통해 숫자 하나를 내뱉는데, 그 값이 0.5를 기준으로 1에 가까우면 질환, 0에 가까우면 정상이라고 할 때 0.51같은 값조차 '잘 모르겠어'라고 대답하지 않고 '이건 질환이야!' 라고 반응한다는 것이다. 이러한 딥러닝의 과감성은 어쩌면 득이 될 수 있지만 어쩌면 독이 될 수도 있다.

요는, 딥러닝 알고리즘을 돌려서 영상의학과 의사보다 더 잘 맞추니 마니 하는 것은 지나치게 나이브한 판단기준이 아닐까 한다. 정말 임상적으로 영상의학과 의사가 대체될 수 있을 만큼의 퍼포먼스를 하려면 - 새로운 가이드라인을 제시하는 능력을 차치하고라도 - 최소한 알고리즘 자신이 무엇에 자신이 있고 무엇에 자신이 없는지를 알려주어야 하지 않을까, 그리고 나서 성능이니 영상의학과가 망하느니 마니 하는 이야기를 는게 맞지 않을까 한다.

P.S) 본문의 문제의식에 대한 답은 어쩌면 Baysian Deep Learning에 있을지도?ㅎㅎ
